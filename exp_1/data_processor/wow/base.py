import string
from collections import Counter
import nltk
import re
from nltk.stem.wordnet import WordNetLemmatizer
from tqdm import tqdm
import general_files.utils.common_util as utils
from general_files.utils.common_util import Result
from general_files.utils.data_util import (
    extract_en_keywords_by_sklearn_tfidf,
    flat,
)
import spacy
from data.wizard_of_wikipedia.basic_preprocess import Processor as BasicProcessor


log = utils.get_logger(__name__)


class Processor(BasicProcessor):
    def __init__(self, config, tokenizer, only_test):
        super(Processor, self).__init__(config, tokenizer, only_test)

    def get_rows(self, all_rows, stage):
        rows = Result()
        bos_token = self.tokenizer.bos_token
        eos_token = self.tokenizer.eos_token
        sep_token = self.tokenizer.sep_token
        user_token = "<user>"
        bot_token = "<bot>"
        knowledge_token = "<knowledge>"

        for dialog in tqdm(all_rows, desc="Ê†ºÂºèÂåñËæìÂÖ•ËæìÂá∫"):
            for uttr in dialog["utterances"]:
                ###############################################
                # Âü∫Á°ÄÊï∞ÊçÆÂ§ÑÁêÜ
                ###############################################
                all_history = []
                for i, h in enumerate(uttr["history"][: : -1]):
                    if i % 2 == 0:
                        all_history.append("<user> " + h)
                    else:
                        all_history.append("<bot> " + h)
                history = flat(all_history[-self.config.history_len:])
                knowledge = flat([knowledge_token, know_segments])
                response = uttr["response"]
                
                ###############################################
                # ÊûÑÂª∫Ê®°ÂûãËæìÂÖ•ËæìÂá∫Ê†ºÂºè
                ###############################################
                text_map = {
                    "k": knowledge,
                    "h": history,
                    "r": response,
                }
                
                input = flat([text_map[p] for p in self.config.input_shape.split('-')])

                target = flat([text_map[p] for p in self.config.target_shape.split('-')])

                decoder_input = flat([bos_token, target])

                row = Result(
                    source=input,
                    target=target,
                    # >>> other_features <<<
                    decoder_input=decoder_input,
                    response=response,
                    knowledge=knowledge,
                    history=history,
                )
                rows.append_values(row)
        return rows

    def tokenize_data(self, batch, stage=None):
        result = Result()
        # ÂêàÂπ∂ÂêåÁ±ªÁºñÁ†ÅÊñπÂºè
        result.merge_or_update(
            self.tokenizer(
                # key ÂØπÂ∫îÁºñÁ†Å‰πãÂêéÁöÑÂ≠óÊÆµÂêçÔºåvalue ÂØπÂ∫îÂéüÂßãÊï∞ÊçÆ‰∏≠ÁöÑÂ≠óÊÆµÂêç
                {
                    "input_ids": batch["source"],
                    "decoder_input_ids": batch["decoder_input"],
                    "labels": batch["target"],
                    "decoder_response": batch["response"],
                    "decoder_knowledge": batch["knowledge"],
                    "decoder_history": batch["history"],
                },
                padding="max_length",
                max_length=self.config.encoder_max_length,
                truncation=True,
                only_input_ids=True,
                add_special_tokens=True,
            )
        )
        # >>> other_features <<<
        # üì¢  ÂèØ‰ª•ÊåâÁÖßËá™Â∑±ÈúÄÊ±ÇÂÆö‰πâÊõ¥Âä†Ëá™Áî±ÁöÑÁºñÁ†ÅÊñπÂºè
        # üì¢  ‰æãÂ¶ÇÔºö‰∏çËøõË°å padÔºåÂè™ÁºñÁ†Å
        # result.merge_or_update(
        #     self.tokenizer(
        #         {
        #             "decoder_other_features": batch["other_features"],
        #         },
        #         truncation=True,
        #         only_input_ids=True,
        #         add_special_tokens=False,
        #     )
        # )
        # üì¢  ‰æãÂ¶ÇÔºöÂè™ËøõË°å padÔºå‰∏çÁºñÁ†Å
        # result.merge_or_update(
        #     self.tokenizer.pad(
        #         {
        #             "decoder_other_features": batch["other_features"],
        #         },
        #         max_length=self.config.encoder_max_length,
        #         truncation=True,
        #     )
        # )
        # üì¢  ‰æãÂ¶ÇÔºöÁõ¥Êé•‰ΩøÁî®ÂéüÂßãÊï∞ÊçÆ
        # result.add(decoder_other_features=batch["other_features"])
        return result
